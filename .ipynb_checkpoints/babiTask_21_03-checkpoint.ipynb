{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECUPERATION DES DONNEES\n",
    "Le code suivant permet de récupérer et parser les données.<br>\n",
    "Source: https://github.com/fchollet/keras/blob/master/examples/babi_rnn.py <br>\n",
    "Note: Nous écrirons notre propre parser par la suite, nous avons repris celui donné en exemple afin de gagner\n",
    "du temps pour ce notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Vers une réponse à une question complète sur l'AI: un ensemble de tâches préalables pour les jouets\"\n",
    "http://arxiv.org/abs/1502.05698 <br>\n",
    "Pour les ressources liées au projet bAbI, se référer à:\n",
    "https://research.facebook.com/researchers/1543934539189348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "from keras.layers import recurrent\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    '''Renvoie les jetons d'une phrase, y compris la ponctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parseur des Histoires fournies dans le format de bAbi tasks\n",
    "    Si only_supporting est true,\n",
    "    seules les phrases qui soutiennent la réponse sont conservées.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            substory = None\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''Given a file name, read the file, retrieve the stories,\n",
    "    \n",
    "    Étant donné un nom de fichier, lisez le fichier, récupérez les histoires,\n",
    "    puis convertissez les phrases en une seule histoire.\n",
    "    Si max_length est fourni,\n",
    "    les histoires plus longues que les jetons max_length seront ignorées.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data \n",
    "            if not max_length or len(flatten(story)) < max_length]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du reseau de neuronnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN:  <class 'keras.layers.recurrent.LSTM'> Embed:  50 Sentences:  100 Query:  100\n"
     ]
    }
   ],
   "source": [
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "QUERY_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "\n",
    "print('RNN: ', RNN, 'Embed: ',EMBED_HIDDEN_SIZE, 'Sentences: ',SENT_HIDDEN_SIZE, 'Query: ', QUERY_HIDDEN_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Téléchargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n",
    "except:\n",
    "    print('Erreur de téléchargement, Veillez le télécharger manuellement'\n",
    "          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\\n'\n",
    "          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observons les données de la première tache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hasmiou/anaconda3/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille initiale du vocabulaire 21\n"
     ]
    }
   ],
   "source": [
    "tar = tarfile.open(path)\n",
    "challenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt'\n",
    "\n",
    "train = get_stories(tar.extractfile(challenge.format('train')))\n",
    "#print(train)\n",
    "\n",
    "test = get_stories(tar.extractfile(challenge.format('test')))\n",
    "#print(test)\n",
    "\n",
    "vocab = sorted(reduce(lambda x, y: x | y, (set(story + q + [answer]) for story, q, answer in train + test)))\n",
    "print('Taille initiale du vocabulaire', len(vocab))\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire: 34\n",
      "Nombre de c aractères: 25\n",
      "Nombre de stories (apprentissage) : 1000\n",
      "Nombre de stories (test) : 1000\n"
     ]
    }
   ],
   "source": [
    "def unique(l):\n",
    "    ret = set()\n",
    "    for el in l:\n",
    "        ret.add(el)\n",
    "    return list(ret)\n",
    "\n",
    "flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "caracteres = unique(flatten(map(list,vocab)))\n",
    "#print(caracteres)\n",
    "sentences_from_stories = list(map(lambda d : d[0],train+test))\n",
    "#print(sentences_from_stories)\n",
    "distribution = dict()\n",
    "for s in sentences_from_stories:\n",
    "    len_s = len(s)\n",
    "    if len_s in distribution.keys():\n",
    "        distribution[len_s] +=1\n",
    "    else:\n",
    "        distribution[len_s] = 1\n",
    "        \n",
    "for i in range(100):\n",
    "    if i not in distribution.keys():\n",
    "        distribution[i]=0\n",
    "\n",
    "print(\"Taille du vocabulaire: {}\".format(len(vocab)))\n",
    "print(\"Nombre de c aractères: {}\".format(len(caracteres)))\n",
    "print(\"Nombre de stories (apprentissage) : {}\".format(len(test)))\n",
    "print(\"Nombre de stories (test) : {}\".format(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarque\n",
    "On remarque que la taille du vocabulaire est relativement réduite, en effet les exemples sont assez repétitifs et utilisent peu de mots. Nous n'avons donc pas besoin de le reduire. Cela vaut également pour le nombre de caractères. Notons que nous ne regarderons que les données de la première task pour l'instant mais globalement le vocabulaire et le nombre de caractères restent assez petits sur l'ensemble des tasks.\n",
    "\n",
    "Nos exemples sont répartis par \"story\", c'est à dire que nous avons une histoire (un ensemble de phrases) qui décrit certaines choses, une question sur cette même histoire ainsi que la réponse associée. Nous avons ainsi 1000 stories pour l'ensemble d'apprentissage et le même nombre pour l'ensemble de test. Si l'on préfère compter l'ensemble des phrases pour chaque story nous en avons 74297.\n",
    "\n",
    "Observons désormais la distribution de la longueur des stories en mots:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
