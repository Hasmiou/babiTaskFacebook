{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECUPERATION DES DONNEES\n",
    "Le code suivant permet de récupérer et parser les données.<br>\n",
    "Source: https://github.com/fchollet/keras/blob/master/examples/babi_rnn.py <br>\n",
    "Note: Nous écrirons notre propre parser par la suite, nous avons repris celui donné en exemple afin de gagner\n",
    "du temps pour ce notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Vers une réponse à une question complète sur l'AI: un ensemble de tâches préalables pour les jouets\"\n",
    "http://arxiv.org/abs/1502.05698 <br>\n",
    "Pour les ressources liées au projet bAbI, se référer à:\n",
    "https://research.facebook.com/researchers/1543934539189348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "from keras.layers import recurrent\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    '''Renvoie les jetons d'une phrase, y compris la ponctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parseur des Histoires fournies dans le format de bAbi tasks\n",
    "    Si only_supporting est true,\n",
    "    seules les phrases qui soutiennent la réponse sont conservées.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1) #Identifiant + la phrase (ligne)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t') #Question + answer\n",
    "            q = tokenize(q)\n",
    "            substory = None\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories : trouver ttes les sous histoires\n",
    "                substory = [x for x in story if x] \n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''\n",
    "    Étant donné un nom de fichier, lisez le fichier, récupérez les histoires,\n",
    "    puis convertissez les phrases en une seule histoire.\n",
    "    Si max_length est fourni,\n",
    "    les histoires plus longues que les jetons max_length seront ignorées.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data \n",
    "            if not max_length or len(flatten(story)) < max_length]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Téléchargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\n",
      "11747328/11745123 [==============================] - 61s 5us/step\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n",
    "except:\n",
    "    print('Erreur de téléchargement, Veillez le télécharger manuellement'\n",
    "          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\\n'\n",
    "          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observons les données de la première tache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille initiale du vocabulaire 21\n"
     ]
    }
   ],
   "source": [
    "tar = tarfile.open(path)\n",
    "challenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt'\n",
    "\n",
    "train = get_stories(tar.extractfile(challenge.format('train')))\n",
    "#print(train)\n",
    "\n",
    "test = get_stories(tar.extractfile(challenge.format('test')))\n",
    "#print(test)\n",
    "\n",
    "vocab = sorted(reduce(lambda x, y: x | y, (set(story + q + [answer]) for story, q, answer in train + test)))\n",
    "print('Taille initiale du vocabulaire', len(vocab))\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire: 21\n",
      "Nombre de c aractères: 29\n",
      "Nombre de stories (apprentissage) : 1000\n",
      "Nombre de stories (test) : 1000\n"
     ]
    }
   ],
   "source": [
    "def unique(l):\n",
    "    ret = set()\n",
    "    for el in l:\n",
    "        ret.add(el)\n",
    "    return list(ret)\n",
    "\n",
    "flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "\n",
    "caracteres = unique(flatten(map(list,vocab)))\n",
    "#print(caracteres)\n",
    "\n",
    "sentences_from_stories = list(map(lambda d : d[0],train+test))\n",
    "#print(sentences_from_stories)\n",
    "\n",
    "distribution = dict()\n",
    "for s in sentences_from_stories:\n",
    "    len_s = len(s)\n",
    "    if len_s in distribution.keys():\n",
    "        distribution[len_s] +=1\n",
    "    else:\n",
    "        distribution[len_s] = 1\n",
    "        \n",
    "for i in range(100):\n",
    "    if i not in distribution.keys():\n",
    "        distribution[i]=0\n",
    "\n",
    "print(\"Taille du vocabulaire: {}\".format(len(vocab)))\n",
    "print(\"Nombre de c aractères: {}\".format(len(caracteres)))\n",
    "print(\"Nombre de stories (apprentissage) : {}\".format(len(test)))\n",
    "print(\"Nombre de stories (test) : {}\".format(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarque\n",
    "On remarque que la taille du vocabulaire est relativement réduite, en effet les exemples sont assez repétitifs et utilisent peu de mots. Nous n'avons donc pas besoin de le reduire. Cela vaut également pour le nombre de caractères. Notons que nous ne regarderons que les données de la première task pour l'instant mais globalement le vocabulaire et le nombre de caractères restent assez petits sur l'ensemble des tasks.\n",
    "\n",
    "Nos exemples sont répartis par \"story\", c'est à dire que nous avons une histoire (un ensemble de phrases) qui décrit certaines choses, une question sur cette même histoire ainsi que la réponse associée. Nous avons ainsi 1000 stories pour l'ensemble d'apprentissage et le même nombre pour l'ensemble de test. Si l'on préfère compter l'ensemble des phrases pour chaque story nous en avons 74297.\n",
    "\n",
    "Observons désormais la distribution de la longueur des stories en mots:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74\n",
      " 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0 266 123  11   0   0   0\n",
      "   0   0   0   0   0   0 168 162  63   7   0   0   0   0   0   0   0   0\n",
      " 119 156  92  28   5   0   0   0   0   0   0   0  67 145 114  47  23   4\n",
      "   0   0   0   0   0   0  44 110 134  66  34  11   1   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wOQ3bpbEQ0sUwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff40cf296a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x,y=[],[]\n",
    "els = sorted(distribution.items(),key=(lambda k:k[0]))\n",
    "\n",
    "\n",
    "for ex,ey in els:\n",
    "    x.append(ex)\n",
    "    y.append(ey)\n",
    "\n",
    "x,y = np.asarray(x),np.asarray(y)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "plt.bar(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous observons un grand nombre de petites stories (12-13-14 mots) alors que les stories plus complexes sont moins nombreuses. Plus on augmente de nombre de mots par story, moins il y a de story disponible.\n",
    "\n",
    "Après avoir analysé ces données, il en ressort que la solution première semble être d'utiliser un réseau neuronal récurrent. En effet nous allons devoir garder un historique des phrases que nous allons lire afin de pouvoir répondre à la question par la suite, ce qui correspond à un réseau neuronal récurrent. Tout le problème est que nous avons deux données à traiter. Par exemple: des histoires et des questions (sachant que la réponse sera le résultat attendu pour les deux). Une idée est de d'abord séparer le problème en deux en créant un modèle pour les stories et un modèle pour les questions, on pourra ensuite effectuer un traitement différent sur ces derniers et décider de la manière dont nous fusionnerons ces deux réseaux. Pour ce faire nous pouvons utiliser la couche Merge de keras : https://keras.io/layers/core/#merge ce qui nous donnera un unique modèle à base de réseaux récurrents.\n",
    "\n",
    "Une autre idée serait d'utiliser des \"memory networks\", ces réseaux récemment décrit dans cet article (https://arxiv.org/abs/1410.3916) semblent adaptés à la résolution de ce problème et en particulier afin de raisonner sur différents élements. Par ailleurs, ils ont été créés spécialement pour résoudre ce problème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
